{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode 1 reward total was -21.000000. running mean: -21.000000\n",
      "resetting env. episode 2 reward total was -19.000000. running mean: -20.980000\n",
      "resetting env. episode 3 reward total was -21.000000. running mean: -20.980200\n",
      "resetting env. episode 4 reward total was -20.000000. running mean: -20.970398\n",
      "resetting env. episode 5 reward total was -21.000000. running mean: -20.970694\n",
      "resetting env. episode 6 reward total was -21.000000. running mean: -20.970987\n",
      "resetting env. episode 7 reward total was -20.000000. running mean: -20.961277\n",
      "resetting env. episode 8 reward total was -20.000000. running mean: -20.951664\n",
      "resetting env. episode 9 reward total was -19.000000. running mean: -20.932148\n",
      "resetting env. episode 10 reward total was -21.000000. running mean: -20.932826\n",
      "resetting env. episode 11 reward total was -20.000000. running mean: -20.923498\n",
      "resetting env. episode 12 reward total was -19.000000. running mean: -20.904263\n",
      "resetting env. episode 13 reward total was -20.000000. running mean: -20.895220\n",
      "resetting env. episode 14 reward total was -21.000000. running mean: -20.896268\n",
      "resetting env. episode 15 reward total was -21.000000. running mean: -20.897306\n",
      "resetting env. episode 16 reward total was -21.000000. running mean: -20.898332\n",
      "resetting env. episode 17 reward total was -21.000000. running mean: -20.899349\n",
      "resetting env. episode 18 reward total was -21.000000. running mean: -20.900356\n",
      "resetting env. episode 19 reward total was -21.000000. running mean: -20.901352\n",
      "resetting env. episode 20 reward total was -20.000000. running mean: -20.892339\n",
      "resetting env. episode 21 reward total was -21.000000. running mean: -20.893415\n",
      "resetting env. episode 22 reward total was -20.000000. running mean: -20.884481\n",
      "resetting env. episode 23 reward total was -18.000000. running mean: -20.855636\n",
      "resetting env. episode 24 reward total was -21.000000. running mean: -20.857080\n",
      "resetting env. episode 25 reward total was -20.000000. running mean: -20.848509\n",
      "resetting env. episode 26 reward total was -21.000000. running mean: -20.850024\n",
      "resetting env. episode 27 reward total was -21.000000. running mean: -20.851524\n",
      "resetting env. episode 28 reward total was -21.000000. running mean: -20.853009\n",
      "resetting env. episode 29 reward total was -20.000000. running mean: -20.844478\n",
      "resetting env. episode 30 reward total was -21.000000. running mean: -20.846034\n",
      "resetting env. episode 31 reward total was -21.000000. running mean: -20.847573\n",
      "resetting env. episode 32 reward total was -20.000000. running mean: -20.839098\n",
      "resetting env. episode 33 reward total was -20.000000. running mean: -20.830707\n",
      "resetting env. episode 34 reward total was -21.000000. running mean: -20.832400\n",
      "resetting env. episode 35 reward total was -21.000000. running mean: -20.834076\n",
      "resetting env. episode 36 reward total was -20.000000. running mean: -20.825735\n",
      "resetting env. episode 37 reward total was -21.000000. running mean: -20.827477\n",
      "resetting env. episode 38 reward total was -21.000000. running mean: -20.829203\n",
      "resetting env. episode 39 reward total was -21.000000. running mean: -20.830911\n",
      "resetting env. episode 40 reward total was -20.000000. running mean: -20.822602\n",
      "resetting env. episode 41 reward total was -19.000000. running mean: -20.804376\n",
      "resetting env. episode 42 reward total was -19.000000. running mean: -20.786332\n",
      "resetting env. episode 43 reward total was -21.000000. running mean: -20.788468\n",
      "resetting env. episode 44 reward total was -21.000000. running mean: -20.790584\n",
      "resetting env. episode 45 reward total was -19.000000. running mean: -20.772678\n",
      "resetting env. episode 46 reward total was -19.000000. running mean: -20.754951\n",
      "resetting env. episode 47 reward total was -21.000000. running mean: -20.757402\n",
      "resetting env. episode 48 reward total was -21.000000. running mean: -20.759828\n",
      "resetting env. episode 49 reward total was -20.000000. running mean: -20.752229\n",
      "resetting env. episode 50 reward total was -21.000000. running mean: -20.754707\n",
      "resetting env. episode 51 reward total was -21.000000. running mean: -20.757160\n",
      "resetting env. episode 52 reward total was -20.000000. running mean: -20.749588\n",
      "resetting env. episode 53 reward total was -21.000000. running mean: -20.752092\n",
      "resetting env. episode 54 reward total was -20.000000. running mean: -20.744572\n",
      "resetting env. episode 55 reward total was -21.000000. running mean: -20.747126\n",
      "resetting env. episode 56 reward total was -21.000000. running mean: -20.749655\n",
      "resetting env. episode 57 reward total was -18.000000. running mean: -20.722158\n",
      "resetting env. episode 58 reward total was -21.000000. running mean: -20.724936\n",
      "resetting env. episode 59 reward total was -21.000000. running mean: -20.727687\n",
      "resetting env. episode 60 reward total was -19.000000. running mean: -20.710410\n",
      "resetting env. episode 61 reward total was -21.000000. running mean: -20.713306\n",
      "resetting env. episode 62 reward total was -21.000000. running mean: -20.716173\n",
      "resetting env. episode 63 reward total was -21.000000. running mean: -20.719011\n",
      "resetting env. episode 64 reward total was -20.000000. running mean: -20.711821\n",
      "resetting env. episode 65 reward total was -19.000000. running mean: -20.694703\n",
      "resetting env. episode 66 reward total was -21.000000. running mean: -20.697756\n",
      "resetting env. episode 67 reward total was -20.000000. running mean: -20.690778\n",
      "resetting env. episode 68 reward total was -19.000000. running mean: -20.673871\n",
      "resetting env. episode 69 reward total was -21.000000. running mean: -20.677132\n",
      "resetting env. episode 70 reward total was -21.000000. running mean: -20.680361\n",
      "resetting env. episode 71 reward total was -19.000000. running mean: -20.663557\n",
      "resetting env. episode 72 reward total was -21.000000. running mean: -20.666921\n",
      "resetting env. episode 73 reward total was -21.000000. running mean: -20.670252\n",
      "resetting env. episode 74 reward total was -20.000000. running mean: -20.663550\n",
      "resetting env. episode 75 reward total was -21.000000. running mean: -20.666914\n",
      "resetting env. episode 76 reward total was -20.000000. running mean: -20.660245\n",
      "resetting env. episode 77 reward total was -20.000000. running mean: -20.653643\n",
      "resetting env. episode 78 reward total was -20.000000. running mean: -20.647106\n",
      "resetting env. episode 79 reward total was -20.000000. running mean: -20.640635\n",
      "resetting env. episode 80 reward total was -21.000000. running mean: -20.644229\n",
      "resetting env. episode 81 reward total was -21.000000. running mean: -20.647786\n",
      "resetting env. episode 82 reward total was -19.000000. running mean: -20.631309\n",
      "resetting env. episode 83 reward total was -20.000000. running mean: -20.624996\n",
      "resetting env. episode 84 reward total was -21.000000. running mean: -20.628746\n",
      "resetting env. episode 85 reward total was -21.000000. running mean: -20.632458\n",
      "resetting env. episode 86 reward total was -20.000000. running mean: -20.626134\n",
      "resetting env. episode 87 reward total was -20.000000. running mean: -20.619872\n",
      "resetting env. episode 88 reward total was -19.000000. running mean: -20.603673\n",
      "resetting env. episode 89 reward total was -20.000000. running mean: -20.597637\n",
      "resetting env. episode 90 reward total was -20.000000. running mean: -20.591660\n",
      "resetting env. episode 91 reward total was -21.000000. running mean: -20.595744\n",
      "resetting env. episode 92 reward total was -21.000000. running mean: -20.599786\n",
      "resetting env. episode 93 reward total was -21.000000. running mean: -20.603788\n",
      "resetting env. episode 94 reward total was -20.000000. running mean: -20.597751\n",
      "resetting env. episode 95 reward total was -21.000000. running mean: -20.601773\n",
      "resetting env. episode 96 reward total was -21.000000. running mean: -20.605755\n",
      "resetting env. episode 97 reward total was -20.000000. running mean: -20.599698\n",
      "resetting env. episode 98 reward total was -18.000000. running mean: -20.573701\n",
      "resetting env. episode 99 reward total was -20.000000. running mean: -20.567964\n",
      "resetting env. episode 100 reward total was -20.000000. running mean: -20.562284\n",
      "resetting env. episode 101 reward total was -19.000000. running mean: -20.546661\n",
      "resetting env. episode 102 reward total was -21.000000. running mean: -20.551195\n",
      "resetting env. episode 103 reward total was -19.000000. running mean: -20.535683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode 104 reward total was -21.000000. running mean: -20.540326\n",
      "resetting env. episode 105 reward total was -21.000000. running mean: -20.544923\n",
      "resetting env. episode 106 reward total was -21.000000. running mean: -20.549473\n",
      "resetting env. episode 107 reward total was -21.000000. running mean: -20.553979\n",
      "resetting env. episode 108 reward total was -21.000000. running mean: -20.558439\n",
      "resetting env. episode 109 reward total was -21.000000. running mean: -20.562855\n",
      "resetting env. episode 110 reward total was -21.000000. running mean: -20.567226\n",
      "resetting env. episode 111 reward total was -21.000000. running mean: -20.571554\n",
      "resetting env. episode 112 reward total was -20.000000. running mean: -20.565838\n",
      "resetting env. episode 113 reward total was -21.000000. running mean: -20.570180\n",
      "resetting env. episode 114 reward total was -20.000000. running mean: -20.564478\n",
      "resetting env. episode 115 reward total was -20.000000. running mean: -20.558833\n",
      "resetting env. episode 116 reward total was -21.000000. running mean: -20.563245\n",
      "resetting env. episode 117 reward total was -21.000000. running mean: -20.567612\n",
      "resetting env. episode 118 reward total was -21.000000. running mean: -20.571936\n",
      "resetting env. episode 119 reward total was -20.000000. running mean: -20.566217\n",
      "resetting env. episode 120 reward total was -21.000000. running mean: -20.570555\n",
      "resetting env. episode 121 reward total was -21.000000. running mean: -20.574849\n",
      "resetting env. episode 122 reward total was -21.000000. running mean: -20.579101\n",
      "resetting env. episode 123 reward total was -21.000000. running mean: -20.583310\n",
      "resetting env. episode 124 reward total was -21.000000. running mean: -20.587477\n",
      "resetting env. episode 125 reward total was -20.000000. running mean: -20.581602\n",
      "resetting env. episode 126 reward total was -21.000000. running mean: -20.585786\n",
      "resetting env. episode 127 reward total was -20.000000. running mean: -20.579928\n",
      "resetting env. episode 128 reward total was -21.000000. running mean: -20.584129\n",
      "resetting env. episode 129 reward total was -19.000000. running mean: -20.568287\n",
      "resetting env. episode 130 reward total was -20.000000. running mean: -20.562605\n",
      "resetting env. episode 131 reward total was -21.000000. running mean: -20.566979\n",
      "resetting env. episode 132 reward total was -21.000000. running mean: -20.571309\n",
      "resetting env. episode 133 reward total was -21.000000. running mean: -20.575596\n",
      "resetting env. episode 134 reward total was -21.000000. running mean: -20.579840\n",
      "resetting env. episode 135 reward total was -21.000000. running mean: -20.584041\n",
      "resetting env. episode 136 reward total was -16.000000. running mean: -20.538201\n",
      "resetting env. episode 137 reward total was -18.000000. running mean: -20.512819\n",
      "resetting env. episode 138 reward total was -20.000000. running mean: -20.507691\n",
      "resetting env. episode 139 reward total was -21.000000. running mean: -20.512614\n",
      "resetting env. episode 140 reward total was -18.000000. running mean: -20.487488\n",
      "resetting env. episode 141 reward total was -21.000000. running mean: -20.492613\n",
      "resetting env. episode 142 reward total was -21.000000. running mean: -20.497687\n",
      "resetting env. episode 143 reward total was -20.000000. running mean: -20.492710\n",
      "resetting env. episode 144 reward total was -21.000000. running mean: -20.497783\n",
      "resetting env. episode 145 reward total was -21.000000. running mean: -20.502805\n",
      "resetting env. episode 146 reward total was -20.000000. running mean: -20.497777\n",
      "resetting env. episode 147 reward total was -20.000000. running mean: -20.492799\n",
      "resetting env. episode 148 reward total was -21.000000. running mean: -20.497871\n",
      "resetting env. episode 149 reward total was -20.000000. running mean: -20.492892\n",
      "resetting env. episode 150 reward total was -21.000000. running mean: -20.497963\n",
      "resetting env. episode 151 reward total was -21.000000. running mean: -20.502984\n",
      "resetting env. episode 152 reward total was -21.000000. running mean: -20.507954\n",
      "resetting env. episode 153 reward total was -19.000000. running mean: -20.492874\n",
      "resetting env. episode 154 reward total was -19.000000. running mean: -20.477946\n",
      "resetting env. episode 155 reward total was -19.000000. running mean: -20.463166\n",
      "resetting env. episode 156 reward total was -19.000000. running mean: -20.448535\n",
      "resetting env. episode 157 reward total was -21.000000. running mean: -20.454049\n",
      "resetting env. episode 158 reward total was -20.000000. running mean: -20.449509\n",
      "resetting env. episode 159 reward total was -21.000000. running mean: -20.455014\n",
      "resetting env. episode 160 reward total was -20.000000. running mean: -20.450463\n",
      "resetting env. episode 161 reward total was -19.000000. running mean: -20.435959\n",
      "resetting env. episode 162 reward total was -21.000000. running mean: -20.441599\n",
      "resetting env. episode 163 reward total was -19.000000. running mean: -20.427183\n",
      "resetting env. episode 164 reward total was -19.000000. running mean: -20.412911\n",
      "resetting env. episode 165 reward total was -20.000000. running mean: -20.408782\n",
      "resetting env. episode 166 reward total was -21.000000. running mean: -20.414694\n",
      "resetting env. episode 167 reward total was -21.000000. running mean: -20.420548\n",
      "resetting env. episode 168 reward total was -19.000000. running mean: -20.406342\n",
      "resetting env. episode 169 reward total was -21.000000. running mean: -20.412279\n",
      "resetting env. episode 170 reward total was -21.000000. running mean: -20.418156\n",
      "resetting env. episode 171 reward total was -20.000000. running mean: -20.413974\n",
      "resetting env. episode 172 reward total was -18.000000. running mean: -20.389835\n",
      "resetting env. episode 173 reward total was -21.000000. running mean: -20.395936\n",
      "resetting env. episode 174 reward total was -19.000000. running mean: -20.381977\n",
      "resetting env. episode 175 reward total was -20.000000. running mean: -20.378157\n",
      "resetting env. episode 176 reward total was -21.000000. running mean: -20.384376\n",
      "resetting env. episode 177 reward total was -21.000000. running mean: -20.390532\n",
      "resetting env. episode 178 reward total was -20.000000. running mean: -20.386626\n",
      "resetting env. episode 179 reward total was -20.000000. running mean: -20.382760\n",
      "resetting env. episode 180 reward total was -19.000000. running mean: -20.368933\n",
      "resetting env. episode 181 reward total was -19.000000. running mean: -20.355243\n",
      "resetting env. episode 182 reward total was -19.000000. running mean: -20.341691\n",
      "resetting env. episode 183 reward total was -21.000000. running mean: -20.348274\n",
      "resetting env. episode 184 reward total was -19.000000. running mean: -20.334791\n",
      "resetting env. episode 185 reward total was -21.000000. running mean: -20.341443\n",
      "resetting env. episode 186 reward total was -21.000000. running mean: -20.348029\n",
      "resetting env. episode 187 reward total was -20.000000. running mean: -20.344549\n",
      "resetting env. episode 188 reward total was -19.000000. running mean: -20.331103\n",
      "resetting env. episode 189 reward total was -21.000000. running mean: -20.337792\n",
      "resetting env. episode 190 reward total was -19.000000. running mean: -20.324414\n",
      "resetting env. episode 191 reward total was -21.000000. running mean: -20.331170\n",
      "resetting env. episode 192 reward total was -19.000000. running mean: -20.317858\n",
      "resetting env. episode 193 reward total was -19.000000. running mean: -20.304680\n",
      "resetting env. episode 194 reward total was -21.000000. running mean: -20.311633\n",
      "resetting env. episode 195 reward total was -20.000000. running mean: -20.308517\n",
      "resetting env. episode 196 reward total was -21.000000. running mean: -20.315431\n",
      "resetting env. episode 197 reward total was -21.000000. running mean: -20.322277\n",
      "resetting env. episode 198 reward total was -20.000000. running mean: -20.319054\n",
      "resetting env. episode 199 reward total was -20.000000. running mean: -20.315864\n",
      "resetting env. episode 200 reward total was -21.000000. running mean: -20.322705\n",
      "resetting env. episode 201 reward total was -19.000000. running mean: -20.309478\n",
      "resetting env. episode 202 reward total was -20.000000. running mean: -20.306383\n",
      "resetting env. episode 203 reward total was -20.000000. running mean: -20.303319\n",
      "resetting env. episode 204 reward total was -20.000000. running mean: -20.300286\n",
      "resetting env. episode 205 reward total was -21.000000. running mean: -20.307283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode 206 reward total was -21.000000. running mean: -20.314211\n",
      "resetting env. episode 207 reward total was -20.000000. running mean: -20.311068\n",
      "resetting env. episode 208 reward total was -21.000000. running mean: -20.317958\n",
      "resetting env. episode 209 reward total was -21.000000. running mean: -20.324778\n",
      "resetting env. episode 210 reward total was -20.000000. running mean: -20.321530\n",
      "resetting env. episode 211 reward total was -21.000000. running mean: -20.328315\n",
      "resetting env. episode 212 reward total was -21.000000. running mean: -20.335032\n",
      "resetting env. episode 213 reward total was -21.000000. running mean: -20.341682\n",
      "resetting env. episode 214 reward total was -21.000000. running mean: -20.348265\n",
      "resetting env. episode 215 reward total was -21.000000. running mean: -20.354782\n",
      "resetting env. episode 216 reward total was -21.000000. running mean: -20.361234\n",
      "resetting env. episode 217 reward total was -20.000000. running mean: -20.357622\n",
      "resetting env. episode 218 reward total was -21.000000. running mean: -20.364046\n",
      "resetting env. episode 219 reward total was -21.000000. running mean: -20.370405\n",
      "resetting env. episode 220 reward total was -20.000000. running mean: -20.366701\n",
      "resetting env. episode 221 reward total was -20.000000. running mean: -20.363034\n",
      "resetting env. episode 222 reward total was -21.000000. running mean: -20.369404\n",
      "resetting env. episode 223 reward total was -21.000000. running mean: -20.375710\n",
      "resetting env. episode 224 reward total was -19.000000. running mean: -20.361953\n",
      "resetting env. episode 225 reward total was -20.000000. running mean: -20.358333\n",
      "resetting env. episode 226 reward total was -21.000000. running mean: -20.364750\n",
      "resetting env. episode 227 reward total was -21.000000. running mean: -20.371102\n",
      "resetting env. episode 228 reward total was -21.000000. running mean: -20.377391\n",
      "resetting env. episode 229 reward total was -21.000000. running mean: -20.383617\n",
      "resetting env. episode 230 reward total was -19.000000. running mean: -20.369781\n",
      "resetting env. episode 231 reward total was -17.000000. running mean: -20.336084\n",
      "resetting env. episode 232 reward total was -18.000000. running mean: -20.312723\n",
      "resetting env. episode 233 reward total was -17.000000. running mean: -20.279595\n",
      "resetting env. episode 234 reward total was -20.000000. running mean: -20.276799\n",
      "resetting env. episode 235 reward total was -20.000000. running mean: -20.274031\n",
      "resetting env. episode 236 reward total was -21.000000. running mean: -20.281291\n",
      "resetting env. episode 237 reward total was -19.000000. running mean: -20.268478\n",
      "resetting env. episode 238 reward total was -21.000000. running mean: -20.275793\n",
      "resetting env. episode 239 reward total was -21.000000. running mean: -20.283036\n",
      "resetting env. episode 240 reward total was -19.000000. running mean: -20.270205\n",
      "resetting env. episode 241 reward total was -20.000000. running mean: -20.267503\n",
      "resetting env. episode 242 reward total was -21.000000. running mean: -20.274828\n",
      "resetting env. episode 243 reward total was -18.000000. running mean: -20.252080\n",
      "resetting env. episode 244 reward total was -20.000000. running mean: -20.249559\n",
      "resetting env. episode 245 reward total was -21.000000. running mean: -20.257063\n",
      "resetting env. episode 246 reward total was -20.000000. running mean: -20.254493\n",
      "resetting env. episode 247 reward total was -19.000000. running mean: -20.241948\n",
      "resetting env. episode 248 reward total was -19.000000. running mean: -20.229528\n",
      "resetting env. episode 249 reward total was -21.000000. running mean: -20.237233\n",
      "resetting env. episode 250 reward total was -21.000000. running mean: -20.244861\n",
      "resetting env. episode 251 reward total was -20.000000. running mean: -20.242412\n",
      "resetting env. episode 252 reward total was -18.000000. running mean: -20.219988\n",
      "resetting env. episode 253 reward total was -21.000000. running mean: -20.227788\n",
      "resetting env. episode 254 reward total was -19.000000. running mean: -20.215510\n",
      "resetting env. episode 255 reward total was -18.000000. running mean: -20.193355\n",
      "resetting env. episode 256 reward total was -21.000000. running mean: -20.201422\n",
      "resetting env. episode 257 reward total was -21.000000. running mean: -20.209407\n",
      "resetting env. episode 258 reward total was -21.000000. running mean: -20.217313\n",
      "resetting env. episode 259 reward total was -20.000000. running mean: -20.215140\n",
      "resetting env. episode 260 reward total was -21.000000. running mean: -20.222989\n",
      "resetting env. episode 261 reward total was -21.000000. running mean: -20.230759\n",
      "resetting env. episode 262 reward total was -21.000000. running mean: -20.238451\n",
      "resetting env. episode 263 reward total was -21.000000. running mean: -20.246067\n",
      "resetting env. episode 264 reward total was -21.000000. running mean: -20.253606\n",
      "resetting env. episode 265 reward total was -20.000000. running mean: -20.251070\n",
      "resetting env. episode 266 reward total was -21.000000. running mean: -20.258559\n",
      "resetting env. episode 267 reward total was -20.000000. running mean: -20.255974\n",
      "resetting env. episode 268 reward total was -18.000000. running mean: -20.233414\n",
      "resetting env. episode 269 reward total was -20.000000. running mean: -20.231080\n",
      "resetting env. episode 270 reward total was -21.000000. running mean: -20.238769\n",
      "resetting env. episode 271 reward total was -21.000000. running mean: -20.246381\n",
      "resetting env. episode 272 reward total was -20.000000. running mean: -20.243918\n",
      "resetting env. episode 273 reward total was -21.000000. running mean: -20.251478\n",
      "resetting env. episode 274 reward total was -21.000000. running mean: -20.258964\n",
      "resetting env. episode 275 reward total was -20.000000. running mean: -20.256374\n",
      "resetting env. episode 276 reward total was -19.000000. running mean: -20.243810\n",
      "resetting env. episode 277 reward total was -21.000000. running mean: -20.251372\n",
      "resetting env. episode 278 reward total was -21.000000. running mean: -20.258858\n",
      "resetting env. episode 279 reward total was -20.000000. running mean: -20.256270\n",
      "resetting env. episode 280 reward total was -21.000000. running mean: -20.263707\n",
      "resetting env. episode 281 reward total was -19.000000. running mean: -20.251070\n",
      "resetting env. episode 282 reward total was -21.000000. running mean: -20.258559\n",
      "resetting env. episode 283 reward total was -21.000000. running mean: -20.265974\n",
      "resetting env. episode 284 reward total was -19.000000. running mean: -20.253314\n",
      "resetting env. episode 285 reward total was -21.000000. running mean: -20.260781\n",
      "resetting env. episode 286 reward total was -21.000000. running mean: -20.268173\n",
      "resetting env. episode 287 reward total was -19.000000. running mean: -20.255491\n",
      "resetting env. episode 288 reward total was -21.000000. running mean: -20.262936\n",
      "resetting env. episode 289 reward total was -20.000000. running mean: -20.260307\n",
      "resetting env. episode 290 reward total was -21.000000. running mean: -20.267704\n",
      "resetting env. episode 291 reward total was -19.000000. running mean: -20.255027\n",
      "resetting env. episode 292 reward total was -21.000000. running mean: -20.262477\n",
      "resetting env. episode 293 reward total was -21.000000. running mean: -20.269852\n",
      "resetting env. episode 294 reward total was -21.000000. running mean: -20.277153\n",
      "resetting env. episode 295 reward total was -21.000000. running mean: -20.284382\n",
      "resetting env. episode 296 reward total was -21.000000. running mean: -20.291538\n",
      "resetting env. episode 297 reward total was -20.000000. running mean: -20.288623\n",
      "resetting env. episode 298 reward total was -21.000000. running mean: -20.295736\n",
      "resetting env. episode 299 reward total was -21.000000. running mean: -20.302779\n",
      "resetting env. episode 300 reward total was -21.000000. running mean: -20.309751\n",
      "resetting env. episode 301 reward total was -19.000000. running mean: -20.296654\n",
      "resetting env. episode 302 reward total was -20.000000. running mean: -20.293687\n",
      "resetting env. episode 303 reward total was -21.000000. running mean: -20.300750\n",
      "resetting env. episode 304 reward total was -19.000000. running mean: -20.287743\n",
      "resetting env. episode 305 reward total was -20.000000. running mean: -20.284865\n",
      "resetting env. episode 306 reward total was -21.000000. running mean: -20.292017\n",
      "resetting env. episode 307 reward total was -21.000000. running mean: -20.299097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode 308 reward total was -20.000000. running mean: -20.296106\n",
      "resetting env. episode 309 reward total was -21.000000. running mean: -20.303145\n",
      "resetting env. episode 310 reward total was -21.000000. running mean: -20.310113\n",
      "resetting env. episode 311 reward total was -20.000000. running mean: -20.307012\n",
      "resetting env. episode 312 reward total was -19.000000. running mean: -20.293942\n",
      "resetting env. episode 313 reward total was -21.000000. running mean: -20.301002\n",
      "resetting env. episode 314 reward total was -20.000000. running mean: -20.297992\n",
      "resetting env. episode 315 reward total was -21.000000. running mean: -20.305013\n",
      "resetting env. episode 316 reward total was -21.000000. running mean: -20.311962\n",
      "resetting env. episode 317 reward total was -20.000000. running mean: -20.308843\n",
      "resetting env. episode 318 reward total was -20.000000. running mean: -20.305754\n",
      "resetting env. episode 319 reward total was -21.000000. running mean: -20.312697\n",
      "resetting env. episode 320 reward total was -21.000000. running mean: -20.319570\n",
      "resetting env. episode 321 reward total was -20.000000. running mean: -20.316374\n",
      "resetting env. episode 322 reward total was -20.000000. running mean: -20.313210\n",
      "resetting env. episode 323 reward total was -21.000000. running mean: -20.320078\n",
      "resetting env. episode 324 reward total was -20.000000. running mean: -20.316878\n",
      "resetting env. episode 325 reward total was -21.000000. running mean: -20.323709\n",
      "resetting env. episode 326 reward total was -20.000000. running mean: -20.320472\n",
      "resetting env. episode 327 reward total was -21.000000. running mean: -20.327267\n",
      "resetting env. episode 328 reward total was -21.000000. running mean: -20.333994\n",
      "resetting env. episode 329 reward total was -21.000000. running mean: -20.340654\n",
      "resetting env. episode 330 reward total was -19.000000. running mean: -20.327248\n",
      "resetting env. episode 331 reward total was -21.000000. running mean: -20.333975\n",
      "resetting env. episode 332 reward total was -20.000000. running mean: -20.330636\n",
      "resetting env. episode 333 reward total was -21.000000. running mean: -20.337329\n",
      "resetting env. episode 334 reward total was -21.000000. running mean: -20.343956\n",
      "resetting env. episode 335 reward total was -20.000000. running mean: -20.340516\n",
      "resetting env. episode 336 reward total was -19.000000. running mean: -20.327111\n",
      "resetting env. episode 337 reward total was -20.000000. running mean: -20.323840\n",
      "resetting env. episode 338 reward total was -20.000000. running mean: -20.320602\n",
      "resetting env. episode 339 reward total was -20.000000. running mean: -20.317396\n",
      "resetting env. episode 340 reward total was -20.000000. running mean: -20.314222\n",
      "resetting env. episode 341 reward total was -21.000000. running mean: -20.321079\n",
      "resetting env. episode 342 reward total was -20.000000. running mean: -20.317869\n",
      "resetting env. episode 343 reward total was -20.000000. running mean: -20.314690\n",
      "resetting env. episode 344 reward total was -19.000000. running mean: -20.301543\n",
      "resetting env. episode 345 reward total was -19.000000. running mean: -20.288528\n",
      "resetting env. episode 346 reward total was -21.000000. running mean: -20.295642\n",
      "resetting env. episode 347 reward total was -21.000000. running mean: -20.302686\n",
      "resetting env. episode 348 reward total was -21.000000. running mean: -20.309659\n",
      "resetting env. episode 349 reward total was -20.000000. running mean: -20.306563\n",
      "resetting env. episode 350 reward total was -21.000000. running mean: -20.313497\n",
      "resetting env. episode 351 reward total was -20.000000. running mean: -20.310362\n",
      "resetting env. episode 352 reward total was -20.000000. running mean: -20.307258\n",
      "resetting env. episode 353 reward total was -21.000000. running mean: -20.314186\n",
      "resetting env. episode 354 reward total was -21.000000. running mean: -20.321044\n",
      "resetting env. episode 355 reward total was -21.000000. running mean: -20.327833\n",
      "resetting env. episode 356 reward total was -18.000000. running mean: -20.304555\n",
      "resetting env. episode 357 reward total was -20.000000. running mean: -20.301510\n",
      "resetting env. episode 358 reward total was -19.000000. running mean: -20.288494\n",
      "resetting env. episode 359 reward total was -21.000000. running mean: -20.295610\n",
      "resetting env. episode 360 reward total was -21.000000. running mean: -20.302653\n",
      "resetting env. episode 361 reward total was -21.000000. running mean: -20.309627\n",
      "resetting env. episode 362 reward total was -21.000000. running mean: -20.316531\n",
      "resetting env. episode 363 reward total was -17.000000. running mean: -20.283365\n",
      "resetting env. episode 364 reward total was -21.000000. running mean: -20.290532\n",
      "resetting env. episode 365 reward total was -20.000000. running mean: -20.287626\n",
      "resetting env. episode 366 reward total was -20.000000. running mean: -20.284750\n",
      "resetting env. episode 367 reward total was -21.000000. running mean: -20.291903\n",
      "resetting env. episode 368 reward total was -21.000000. running mean: -20.298984\n",
      "resetting env. episode 369 reward total was -21.000000. running mean: -20.305994\n",
      "resetting env. episode 370 reward total was -20.000000. running mean: -20.302934\n",
      "resetting env. episode 371 reward total was -21.000000. running mean: -20.309904\n",
      "resetting env. episode 372 reward total was -19.000000. running mean: -20.296805\n",
      "resetting env. episode 373 reward total was -21.000000. running mean: -20.303837\n",
      "resetting env. episode 374 reward total was -21.000000. running mean: -20.310799\n",
      "resetting env. episode 375 reward total was -21.000000. running mean: -20.317691\n",
      "resetting env. episode 376 reward total was -20.000000. running mean: -20.314514\n",
      "resetting env. episode 377 reward total was -20.000000. running mean: -20.311369\n",
      "resetting env. episode 378 reward total was -19.000000. running mean: -20.298255\n",
      "resetting env. episode 379 reward total was -21.000000. running mean: -20.305273\n",
      "resetting env. episode 380 reward total was -21.000000. running mean: -20.312220\n",
      "resetting env. episode 381 reward total was -20.000000. running mean: -20.309098\n",
      "resetting env. episode 382 reward total was -20.000000. running mean: -20.306007\n",
      "resetting env. episode 383 reward total was -21.000000. running mean: -20.312947\n",
      "resetting env. episode 384 reward total was -21.000000. running mean: -20.319817\n",
      "resetting env. episode 385 reward total was -20.000000. running mean: -20.316619\n",
      "resetting env. episode 386 reward total was -21.000000. running mean: -20.323453\n",
      "resetting env. episode 387 reward total was -20.000000. running mean: -20.320218\n",
      "resetting env. episode 388 reward total was -20.000000. running mean: -20.317016\n",
      "resetting env. episode 389 reward total was -21.000000. running mean: -20.323846\n",
      "resetting env. episode 390 reward total was -20.000000. running mean: -20.320608\n",
      "resetting env. episode 391 reward total was -20.000000. running mean: -20.317401\n",
      "resetting env. episode 392 reward total was -21.000000. running mean: -20.324227\n",
      "resetting env. episode 393 reward total was -19.000000. running mean: -20.310985\n",
      "resetting env. episode 394 reward total was -21.000000. running mean: -20.317875\n",
      "resetting env. episode 395 reward total was -21.000000. running mean: -20.324697\n",
      "resetting env. episode 396 reward total was -21.000000. running mean: -20.331450\n",
      "resetting env. episode 397 reward total was -21.000000. running mean: -20.338135\n",
      "resetting env. episode 398 reward total was -20.000000. running mean: -20.334754\n",
      "resetting env. episode 399 reward total was -19.000000. running mean: -20.321406\n",
      "resetting env. episode 400 reward total was -21.000000. running mean: -20.328192\n",
      "resetting env. episode 401 reward total was -20.000000. running mean: -20.324910\n",
      "resetting env. episode 402 reward total was -21.000000. running mean: -20.331661\n",
      "resetting env. episode 403 reward total was -21.000000. running mean: -20.338345\n",
      "resetting env. episode 404 reward total was -21.000000. running mean: -20.344961\n",
      "resetting env. episode 405 reward total was -21.000000. running mean: -20.351511\n",
      "resetting env. episode 406 reward total was -19.000000. running mean: -20.337996\n",
      "resetting env. episode 407 reward total was -21.000000. running mean: -20.344616\n",
      "resetting env. episode 408 reward total was -21.000000. running mean: -20.351170\n",
      "resetting env. episode 409 reward total was -21.000000. running mean: -20.357659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode 410 reward total was -21.000000. running mean: -20.364082\n",
      "resetting env. episode 411 reward total was -19.000000. running mean: -20.350441\n",
      "resetting env. episode 412 reward total was -20.000000. running mean: -20.346937\n",
      "resetting env. episode 413 reward total was -21.000000. running mean: -20.353467\n",
      "resetting env. episode 414 reward total was -21.000000. running mean: -20.359933\n",
      "resetting env. episode 415 reward total was -21.000000. running mean: -20.366333\n",
      "resetting env. episode 416 reward total was -20.000000. running mean: -20.362670\n",
      "resetting env. episode 417 reward total was -19.000000. running mean: -20.349043\n",
      "resetting env. episode 418 reward total was -21.000000. running mean: -20.355553\n",
      "resetting env. episode 419 reward total was -21.000000. running mean: -20.361997\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import gym\n",
    "\n",
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = True # resume from previous checkpoint?\n",
    "render = True\n",
    "\n",
    "# model initialization\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "  model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "  model = {}\n",
    "  model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "  model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "  \n",
    "\n",
    "grad_buffer = { k : np.zeros_like(v) for (k,v) in model.items() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for (k,v) in model.items() } # rmsprop memory\n",
    "\n",
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "  I = I[35:195] # crop\n",
    "  I = I[::2,::2,0] # downsample by factor of 2\n",
    "  I[I == 144] = 0 # erase background (background type 1)\n",
    "  I[I == 109] = 0 # erase background (background type 2)\n",
    "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(range(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 # ReLU nonlinearity\n",
    "  logp = np.dot(model['W2'], h)\n",
    "  p = sigmoid(logp)\n",
    "  return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "  dh = np.outer(epdlogp, model['W2'])\n",
    "  dh[eph <= 0] = 0 # backpro prelu\n",
    "  dW1 = np.dot(dh.T, epx)\n",
    "  return {'W1':dW1, 'W2':dW2}\n",
    "\n",
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "while True:\n",
    "  if render: env.render()\n",
    "\n",
    "  # preprocess the observation, set input to network to be difference image\n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "  prev_x = cur_x\n",
    "\n",
    "  # forward the policy network and sample an action from the returned probability\n",
    "  aprob, h = policy_forward(x)\n",
    "  action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "\n",
    "  # record various intermediates (needed later for backprop)\n",
    "  xs.append(x) # observation\n",
    "  hs.append(h) # hidden state\n",
    "  y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "  dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "\n",
    "  # step the environment and get new measurements\n",
    "  observation, reward, done, info = env.step(action)\n",
    "  reward_sum += reward\n",
    "\n",
    "  drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "  if done: # an episode finished\n",
    "    episode_number += 1\n",
    "    # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "    epx = np.vstack(xs)\n",
    "    eph = np.vstack(hs)\n",
    "    epdlogp = np.vstack(dlogps)\n",
    "    epr = np.vstack(drs)\n",
    "    xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "    # compute the discounted reward backwards through time\n",
    "    discounted_epr = discount_rewards(epr)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    discounted_epr -= np.mean(discounted_epr)\n",
    "    discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "    epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "    grad = policy_backward(eph, epdlogp)\n",
    "    for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "    # perform rmsprop parameter update every batch_size episodes\n",
    "    if episode_number % batch_size == 0:\n",
    "      for (k,v) in model.items():\n",
    "        g = grad_buffer[k] # gradient\n",
    "        rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "        model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "        grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "    # boring book-keeping\n",
    "    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "    print( 'resetting env. episode %d reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
    "    if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "    reward_sum = 0\n",
    "    observation = env.reset() # reset env\n",
    "    prev_x = None\n",
    "\n",
    "#  if reward > 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
    "#    print ('ep %d: game finished, reward: %f %s' % (episode_number, reward, ('' if reward == -1 else ' !!!!!!!!')))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
